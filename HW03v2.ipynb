{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ad4ef7",
   "metadata": {},
   "source": [
    "Домашняя работа №3 по курсу \"Продвинутое машинное обучение\"\n",
    "\n",
    "выполнил Денисов Артем, MADE-ML-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac049384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed, choices, random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4987bcc",
   "metadata": {},
   "source": [
    "## 0. Подготовка данных и вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "131a179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя' + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4f28e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_text_file(filepath):\n",
    "    \"\"\"\n",
    "    открытие файла с текстом и преобразование его в 1 строку\n",
    "    \"\"\"\n",
    "    text = []\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        for row in file:\n",
    "            text.append(row)\n",
    "    return ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aa586987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, alphabet):\n",
    "    \"\"\"\n",
    "    удаление из текста цифр и знаков препинания\n",
    "    \"\"\"\n",
    "    new_text = []\n",
    "    for letter in text.lower():\n",
    "        if letter in alphabet:\n",
    "            new_text.append(letter)\n",
    "    return ''.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d78db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_accuracy(true_text, decoded_text, alphabet):\n",
    "    \"\"\"\n",
    "    функция для проверки качества модели, считает долю отгаданны букв. \n",
    "    Работает при условии, что известен правильный вариант расшифровки\n",
    "    \"\"\"\n",
    "    all_answers = 0\n",
    "    correct_answers = 0\n",
    "    for letter_true, letter_decoded in zip(true_text, decoded_text):\n",
    "        if letter_decoded in alphabet:\n",
    "            if letter_true == letter_decoded:\n",
    "                correct_answers += 1\n",
    "            all_answers += 1\n",
    "    return correct_answers / all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ebf80b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(717873, 649795)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = open_text_file('WarAndPeace.txt')\n",
    "processed_corpus = preprocess_text(corpus, ALPHABET)\n",
    "train_frequency = build_letter_frequency(processed_corpus, ALPHABET)\n",
    "len(corpus), len(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf1282b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мне теперь так ясна моя мысль  говорил толстой в  году завершая работу над романом  так в анне карениной я люблю мысль семейную в войне и мире любил мысль народную вследствие войны  года конечно и в войне и мире есть семейная хроника и в анне карениной есть картины народной жизни но для каждого романа нужна была своя особенная любовь мысль семейная для х годов  века особенно актуальна одна из самых важных задач в этом текущем для меня например  признавался фм достоевский  молодое поколение и вместе с тем современная русская семья которая я предчувствую это далеко не такова как всего еще двадцать лет назад'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "тестовый кусок текста из предисловия к Анне Карениной, на котором будут проверяться модели\n",
    "\"\"\"\n",
    "test_text = \"«Мне теперь так ясна моя мысль, – говорил Толстой в 1877 году, завершая работу над романом. – Так, в „Анне Карениной“ я люблю мысль семейную, в „Войне и мире“ любил мысль народную вследствие войны 1812 года…».[25] Конечно, и в «Войне и мире» есть «семейная хроника», и в «Анне Карениной» есть «картины народной жизни». Но для каждого романа нужна была своя, особенная любовь. «Мысль семейная» для 70-х годов XIX века особенно актуальна. «Одна из самых важных задач в этом текущем, для меня, например, – признавался Ф.М. Достоевский, – молодое поколение, и вместе с тем современная русская семья, которая, я предчувствую это, далеко не такова, как всего еще двадцать лет назад».[26]\"\n",
    "test_text = preprocess_text(test_text, ALPHABET)\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20ad6907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_text(text, alphabet):\n",
    "    \"\"\"\n",
    "    функция для зашифровки текста\n",
    "    \"\"\"\n",
    "    seed(42)\n",
    "    shuffled_text = []\n",
    "    \n",
    "    shuffled_alphabet_list = [letter for letter in alphabet]\n",
    "    alphabet_list = [letter for letter in alphabet]\n",
    "    shuffle(shuffled_alphabet_list)\n",
    "    shuffle_mapping = {a: b for a, b in zip(alphabet_list, shuffled_alphabet_list)} \n",
    "    for letter in text:\n",
    "        if letter in alphabet:\n",
    "            shuffled_text.append(shuffle_mapping[letter])\n",
    "        else:\n",
    "            shuffled_text.append(letter)\n",
    "    return ''.join(shuffled_text), shuffle_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de7bb982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'изыжяыёыхэжяенжбазежиьбжидашэжжльуьхкшжяьшаяьщжужжльчмжпеуыхфебжхетьямжзечжхьиезьижжяенжужеззыжнехызкзьщжбжшртшржидашэжаыиыщзмржужуьщзыжкжикхыжшрткшжидашэжзехьчзмржуашычаяукыжуьщзджжльчежньзыъзьжкжужуьщзыжкжикхыжыаяэжаыиыщзебжвхьзкнежкжужеззыжнехызкзьщжыаяэжнехякзджзехьчзьщжйкпзкжзьжчшбжнейчьльжхьиезежзмйзежтдшежауьбжьаьтыззебжшртьуэжидашэжаыиыщзебжчшбжвжльчьужжуынежьаьтыззьженямешэзежьчзежкпжаеидвжуейздвжпечеъжуж яьижяынмгыижчшбжиызбжзеёхкиыхжжёхкпзеуешабжсижчьаяьыуанкщжжиьшьчьыжёьньшызкыжкжуиыаяыжажяыижаьухыиыззебжхмаанебжаыиэбжньяьхебжбжёхычъмуаяумрж яьжчешыньжзыжяеньуежненжуаыльжыгыжчуечюеяэжшыяжзепеч'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_text, shuffle_mapping = shuffle_text(test_text, ALPHABET)\n",
    "shuffled_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c45e5",
   "metadata": {},
   "source": [
    "## 1. Реализуйте базовый частотный метод по Шерлоку Холмсу\n",
    "○\tподсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "○\tвозьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "○\tрасшифруйте их таким частотным методом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e016e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_letter_frequency(text, alphabet):\n",
    "    \"\"\"\n",
    "    создание частотного словаря букв на большом тексте\n",
    "    \"\"\"\n",
    "    frequency_dict = {letter: 0 for letter in alphabet}\n",
    "    for letter in text:\n",
    "        frequency_dict[letter] += 1\n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c088b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoding_mapping(train_frequency, test_frequency):\n",
    "    \"\"\"\n",
    "    функция для сопоставления двух алфавитов. Каждый из алфавитом сортируется по частоте, \n",
    "    предполагется, если порядок букв в отсортированном алфавите совпадает, то и буквы алфавитов совпадают\n",
    "    \"\"\"\n",
    "    train_frequency_list = [(value, key) for key, value in train_frequency.items()]\n",
    "    train_frequency_list.sort(reverse=True)\n",
    "    test_frequency_list = [(value, key) for key, value in test_frequency.items()]\n",
    "    test_frequency_list.sort(reverse=True)\n",
    "    decoding_mapping = dict()\n",
    "    for i in range(len(test_frequency_list)):\n",
    "        train_counter, train_letter = train_frequency_list[i]\n",
    "        test_counter, test_letter = test_frequency_list[i]\n",
    "        decoding_mapping[test_letter] = train_letter\n",
    "    return decoding_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "298394a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(text, mapping):\n",
    "    \"\"\"\n",
    "    функция для расшифровки текста при заданном словаре между зашифрованным алфавитом и правильным алфавитом\n",
    "    \"\"\"\n",
    "    decoded_text = []\n",
    "    for letter in text:\n",
    "        decoded_text.append(mapping[letter])\n",
    "    return ''.join(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2ff1add",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frequency = build_letter_frequency(shuffled_text, ALPHABET)\n",
    "decode_mapping = make_decoding_mapping(train_frequency, test_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a612e809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сие лейерп лам униа соу сьнвп  чоторкв ловнлоя т  чодг батерщау раыолг иад росаиос  лам т аиие мареикиоя у взывз сьнвп несеяигз т тояие к скре взыкв сьнвп иародигз тнведнлтке тояиь  чода моиехио к т тояие к скре енлп несеяиау шроикма к т аиие мареикиоя енлп марлкиь иародиоя жкбик ио дву маждочо росаиа игжиа ыьва нтоу оноыеииау взыотп сьнвп несеяиау дву ш чодот  тема оноыеиио амлгавпиа одиа кб насьш тажиьш бадах т цлос лемгюес дву сеиу иайрксер  йркбиатавну фс донлоетнмкя  соводое йомовеике к тсенле н лес нотресеииау ргннмау неспу молорау у йредхгтнлтгз цло давемо ие ламота мам тнечо еюе дтадэалп вел иабад\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode_text(shuffled_text, decode_mapping)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fb22c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.489\n"
     ]
    }
   ],
   "source": [
    "print(f\"{letter_accuracy(test_text, decoded_text, ALPHABET):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf0b5f",
   "metadata": {},
   "source": [
    "Расшифровка текста еще нечитаемая, хотя почти половину букв получилось угадать"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9ef71",
   "metadata": {},
   "source": [
    "## 2.\tБиграммы\n",
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "466e2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_letter_frequency_bigram(text, n=2):\n",
    "    \"\"\"\n",
    "    функция для построения частотного словаря биграмм\n",
    "    \"\"\"\n",
    "    frequency_dict = dict()\n",
    "    for letter_index in range(len(text) - n + 1):\n",
    "        bigram = text[letter_index:letter_index + n]\n",
    "        if bigram in frequency_dict.keys():\n",
    "            frequency_dict[bigram] += 1\n",
    "        else:\n",
    "            frequency_dict[bigram] = 1\n",
    "    return frequency_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "926e166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text_bigram(text, mapping, n=2):\n",
    "    \"\"\"\n",
    "    Не придумал красивого способа для биграмм, похожего на базовую модель. \n",
    "    Предположил, что из словаря биграмм можно определить букву, которая на 0 месте в биграмме\n",
    "    \"\"\"\n",
    "    decoded_text = []\n",
    "    for letter_index in range(len(text) - n + 1):\n",
    "        bigram = text[letter_index:letter_index + n]\n",
    "        decoded_text.append(mapping[bigram][0])\n",
    "    decoded_text.append(mapping[bigram][1])\n",
    "    return ''.join(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd588c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(827, 199)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frequency_bigram = build_letter_frequency_bigram(processed_corpus)\n",
    "test_frequency_bigram = build_letter_frequency_bigram(shuffled_text)\n",
    "len(train_frequency_bigram), len(test_frequency_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a511a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_mapping = make_decoding_mapping(train_frequency_bigram, test_frequency_bigram)\n",
    "decoded_text_bigram = decode_text_bigram(shuffled_text, decode_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e57866cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'яаеное тл нолдбуби  нио оиле накретыввноаюко  аннакяуси дхткорориорелс и шровт гвоннолдане оаеп лйсн г  ботевдез оиле ту ьолаазанае лаев  ичйетевгвв оиле  илоядааза лрсткаисеае леанакя  п нахэг в анае лаев  ичйеоокк ту ьолироавонну  в ане оаеп лйсн г  оокк п лик еа илоядг  ли нн  г маноп мипск ровт и  аз и шдрт тлеиолсмеосоиротевррс  оиле ту ьолироманоадакяпрннахт  лсмеосог ел лымепи ляди в итщуоадавм еади   еаанмновоноттус омано ьсуо игтырьтинжты нидвмюловоомпскочб вяе н начяпчежии арснсев асьокоетано отмрий ьсоирорсыив роту сеоп еотиробожтйсдшк ка азмно м мрт   аенол рв п лда уык омсембв о жк тркс ио  ч'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7d0b2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.074\n"
     ]
    }
   ],
   "source": [
    "alphabet = ''.join(set(decoded_text_bigram))\n",
    "print(f\"{letter_accuracy(test_text, decoded_text_bigram, alphabet):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439791a",
   "metadata": {},
   "source": [
    "Правильно определить частоты биграмм на маленьком куске текста, и я предполагаю, есть более красивый способ расшифровки, и результат получился хуже, чем в базовой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4a2163",
   "metadata": {},
   "source": [
    "## 3. MCMM алгоритм\t\n",
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "\n",
    "○\tпредложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "\n",
    "○\tреализуйте и протестируйте его, убедитесь, что результаты улучшились\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea7ee2",
   "metadata": {},
   "source": [
    "0. Биграммы - это марковская цепь, вторая буква, которая следует за первой и появляется с вероятностью, которую можно оценить\n",
    "1. На корпусе текстов я оцениваю по биграммам, какова вероятность появления буквы i+1 после буквы i. Для этого я считаю количество таких сочетаний, а затем делю это на примерно длину текста, чтобы отнормировать\n",
    "2. Для оценки правдоподобия текста используется функция, где просто перемножаются вероятности каждой из биграмм. Чтобы не оперировать в Python очень маленькими числами на грани ошибки округления, вероятности логарифмируются и складываются\n",
    "3. Расчет начинается со словаря со случайным сопоставлением алфавита зашифрованного сообщения и алфавита, обученного на большом тексте.\n",
    "4. Текст расшифровывается при текущем сопоставлении алфавитов, расчитывается его правдоподобие\n",
    "5. Далее выбираются два случайных симввола и переставляются в словаре\n",
    "6. Текст расшифровывается на попытке перестановки, рассчитывается его правдоподобие. (Первоначально я принимал перестановку, только если правдоподобие росло, но в ходе обучения редко происходили     перестановки и качество получалось очень плохим)\n",
    "7. После этого изменил модель и стал принимать перестановку с вероятностью p(current) / p(best). Перестановки стали случаться чаще и в результаты некоторых обучений модели стали лучше, чем в базовом варианте\n",
    "8. Если перестановка принимается, происходит обновления словаря между двумя алфавитами. Далее происходит переход к пункту 5 и так 15 000 итераций\n",
    "8. Прогоны чаще всего сходились в локальном оптимуме и давали плохое качество модели, чтобы обойти, происходит 30 попыток обучения и в финале принимается результат с максимальным правдоподобием.\n",
    "10. Остановился на количесте итераций для перестановок 15000 - это примерно 600 перестановок и 30 попыток обучения, чтобы получить хотя бы 1 результат близкий к оптимуму"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cd5f8ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilites(corpus, alphabet, ngram=2):\n",
    "    \"\"\"\n",
    "    функция построения частотного словаря биграмм и н-грамм\n",
    "    \"\"\"\n",
    "    n_text = len(corpus)\n",
    "    prob_dict = dict()\n",
    "    for letter_index in range(n_text - ngram):\n",
    "        n_letter = corpus[letter_index:letter_index + ngram]\n",
    "        if n_letter in prob_dict.keys():\n",
    "            prob_dict[n_letter] += 1\n",
    "        else:\n",
    "            prob_dict[n_letter] = 1\n",
    "\n",
    "    \n",
    "    for key in prob_dict.keys():\n",
    "        prob_dict[key] /= (n_text - n_gram)\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "84f3fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_probability(text, prob_dict, ngram=2):\n",
    "    \"\"\"\n",
    "    функция для оценки правдоподобия текста при заданном частотном словаре биграмм\n",
    "    \"\"\"\n",
    "    n_text = len(text)\n",
    "    log_prob = 0\n",
    "    for letter_index in range(n_text - ngram + 1):\n",
    "        n_letters = text[letter_index:letter_index + ngram]\n",
    "        if n_letters in prob_dict.keys():\n",
    "            log_prob += np.log(prob_dict[n_letters])\n",
    "        else:\n",
    "            log_prob += -10000\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fcb1ea5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_dict = estimate_probabilites(processed_corpus, ALPHABET)\n",
    "len(prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0a4366dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text_mcmm(text, prob_dict, alphabet, n_steps=1000, ngram=2):\n",
    "    \"\"\"\n",
    "    функция, в которой происходит 1 попытка обучения с количеством итераций n_steps. \n",
    "    Соответствует шагам 3-8 из описания\n",
    "    \"\"\"\n",
    "    text_alphabet = list(set(text))\n",
    "    if len(text_alphabet) < len(alphabet):\n",
    "        for i in range(len(alphabet) - len(text_alphabet)):\n",
    "            text_alphabet.append(\"pad\" + str(i))\n",
    "    current_log_proba = count_probability(text, prob_dict, ngram=ngram)\n",
    "    mapping_for_decoding = {text_letter: letter for text_letter, letter in zip(text_alphabet, alphabet)}\n",
    "    accepted_steps = 0\n",
    "    for i in range(n_steps):\n",
    "        letter_one, letter_two = choices(text_alphabet, k=2)\n",
    "        test_mapping = mapping_for_decoding.copy()\n",
    "        test_mapping[letter_one], test_mapping[letter_two] = test_mapping[letter_two], test_mapping[letter_one]\n",
    "        try_decoded_text = decode_text(text, test_mapping)\n",
    "        new_prob = count_probability(try_decoded_text, prob_dict, ngram=ngram)\n",
    "        \n",
    "        prob_condition = np.exp(new_prob - current_log_proba)\n",
    "        if prob_condition > random():\n",
    "            accepted_steps += 1\n",
    "            mapping_for_decoding = test_mapping\n",
    "            current_log_proba = new_prob\n",
    "    return mapping_for_decoding, current_log_proba, accepted_steps / n_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8687a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-57df6514c700>:17: RuntimeWarning: overflow encountered in exp\n",
      "  prob_condition = np.exp(new_prob - current_log_proba)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_proba: -3875.84, accept ratio: 0.04\n",
      "log_proba: -3852.71, accept ratio: 0.04\n",
      "log_proba: -3822.23, accept ratio: 0.04\n",
      "log_proba: -3425.65, accept ratio: 0.04\n",
      "log_proba: -3395.59, accept ratio: 0.04\n",
      "мне теперь так ясна моя мысль  говорил толстой в  гобу заверцая радоту наб романом  так в анне карениной я людлю мысль семейную в войне и мире людил мысль наробную вслебствие войны  гоба конечно и в войне и мире есть семейная хроника и в анне карениной есть картины наробной жизни но бля кажбого романа нужна дыла своя осоденная людовь мысль семейная бля х гобов  века осоденно актуальна обна из самых важных забач в этом текушем бля меня например  признавался фм бостоевский  молобое поколение и вместе с тем современная русская семья которая я пребчувствую это балеко не такова как всего еше бвабщать лет назаб\n",
      "0.949\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "код для запуска новых циклов обучения и выбора лучшего из них, соответствует пункту 9 описания\n",
    "\"\"\"\n",
    "best_proba = -100000\n",
    "for i in range(30):\n",
    "    mapping, proba, accepted = decode_text_mcmm(shuffled_text, prob_dict, ALPHABET, n_steps=10000)\n",
    "    if proba > best_proba:\n",
    "        best_proba = proba\n",
    "        best_mapping = mapping\n",
    "        print(f\"log_proba: {proba:0.2f}, accept ratio: {accepted:0.2f}\")\n",
    "decoded_text = decode_text(shuffled_text, best_mapping)    \n",
    "print(decoded_text)\n",
    "accuracy = letter_accuracy(test_text, decoded_text, ALPHABET)\n",
    "print(f\"точность расшифровки:{accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d23507f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Текст из домашней работы, для проверки\n",
    "\"\"\"\n",
    "test = \"დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b0c372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-57df6514c700>:17: RuntimeWarning: overflow encountered in exp\n",
      "  prob_condition = np.exp(new_prob - current_log_proba)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_proba: -1315.34, accept ratio: 0.08\n",
      "log_proba: -1286.20, accept ratio: 0.07\n",
      "log_proba: -1257.11, accept ratio: 0.07\n",
      "если вы вимите норзальный или подти норзальный текст у этого соочшения который легко продитать скорее всего вы все смелали правильно и полудите заксизальный чалл ба послемнее детвертое бамание курса хотя конедно я нидего не очешаю\n"
     ]
    }
   ],
   "source": [
    "best_proba = -10000\n",
    "for i in range(30):\n",
    "    mapping, proba, accepted = decode_text_mcmm(test, prob_dict, ALPHABET, n_steps=10000)\n",
    "    if proba > best_proba:\n",
    "        best_proba = proba\n",
    "        print(f\"log_proba: {proba:0.2f}, accept ratio: {accepted:0.2f}\")\n",
    "        best_mapping = mapping\n",
    "decoded_text = decode_text(test, best_mapping)    \n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7db7e",
   "metadata": {},
   "source": [
    "Со второй попытки получилась достаточно хорошая расшифровка"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
